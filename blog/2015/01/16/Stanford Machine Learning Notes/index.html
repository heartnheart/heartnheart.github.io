<!DOCTYPE html>
<html lang="en-us">
  <head>
    <title>Stanford Machine Learning Notes - org-page</title>
    <meta charset="utf-8" />
    <meta name="author" content="Xiaolong Zhang" />
    <meta name="description" content="This is the notes when I&#39;m learning Stanford Machine Learning on Coursera" />
    <meta name="keywords" content="Machine Learning,notes" />
    <link rel="stylesheet" href="/media/css/main.css" type="text/css">
    <link rel="stylesheet" href="/media/css/prettify.css" type="text/css">
  </head>
  <body class="container">
    <div>
      <header class="masthead">
        <h1 class="masthead-title"><a href="/">org-page</a></h1>
        <p>static site generator</p>
        <ul>
          <li><a href="/blog/">Blog</a></li>
          <li><a href="/notes/">Notes</a></li>
          <li><a href="/tags/">Tags</a></li>
          <li><a href="/about/">About</a></li>
          <li><a href="https://github.com/kelvinh/org-page">GitHub</a></li>
          <li><a href="/rss.xml">RSS</a></li>
        </ul>
        <form method="get" id="searchform" action="http://www.google.com/search">
          <input type="text" class="field" name="q" id="s" placeholder="Search">
          <input type="hidden" name="q" value="site:heartnheart.github.io">
        </form>
      </header>
    </div>

<div>
<div class="post">
<h1>Stanford Machine Learning Notes</h1>
<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Info</h2>
<div class="outline-text-2" id="text-1">
<p>
URL is <a href="https://class.coursera.org/ml-005/lecture">https://class.coursera.org/ml-005/lecture</a>
</p>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">Introduciton</h2>
<div class="outline-text-2" id="text-2">
</div><div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1">What is machine learning</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>Arthur Samuel (1959): <b>Field of study of that gives computers the ability to learn without being explicitly programmed.</b> 
</li>
<li>Tom Mitchel (1998): A Well-posed Learning Problem is *A computer program is said to learn from experience E with repsect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2">Machine learning algorithms:</h3>
<div class="outline-text-3" id="text-2-2">
<p>
We mainly talks about <b>two types of algorithm</b>:
</p>
<ul class="org-ul">
<li>Supervised Learning
</li>
<li>Unsupervised Learning
</li>
<li>Others: Reinforcement Learning, Recommender System.
</li>
</ul>
<p>
And <b>Practical advice for applying learning algorithm</b>
</p>
</div>
<div id="outline-container-sec-2-2-1" class="outline-4">
<h4 id="sec-2-2-1">Supervised Learning</h4>
<div class="outline-text-4" id="text-2-2-1">
<p>
"Right Answers" are given. 
</p>
</div>
<ul class="org-ul"><li><a id="sec-2-2-1-1" name="sec-2-2-1-1"></a>Regression (Housing price prediction)<br  /><div class="outline-text-5" id="text-2-2-1-1">
<p>
Predict continuous valued output (price)
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-01.png" alt="screenshot-01.png" />
</p>
</div>
</li>
<li><a id="sec-2-2-1-2" name="sec-2-2-1-2"></a>Classification (Breast cancer (malignant, benign)<br  /><div class="outline-text-5" id="text-2-2-1-2">
<p>
Discrete valued output (0 or 1)
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-02.png" alt="screenshot-02.png" />
</p>
</div>
</li></ul>
</div>

<div id="outline-container-sec-2-2-2" class="outline-4">
<h4 id="sec-2-2-2">Unsupervised Learning</h4>
<div class="outline-text-4" id="text-2-2-2">
<p>
No knowledge is given.
</p>
</div>
<ul class="org-ul"><li><a id="sec-2-2-2-1" name="sec-2-2-2-1"></a>Google News Grouping<br  /><div class="outline-text-5" id="text-2-2-2-1">

<div class="figure">
<p><img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-03.png" alt="screenshot-03.png" />
</p>
</div>
</div>
</li>
<li><a id="sec-2-2-2-2" name="sec-2-2-2-2"></a>Cocktail party problem<br  /><div class="outline-text-5" id="text-2-2-2-2">

<div class="figure">
<p><img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-04.png" alt="screenshot-04.png" />
</p>
</div>
</div>
</li></ul>
</div>
</div>
</div>
<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3">Linear Regression (Week 1)</h2>
<div class="outline-text-2" id="text-3">
</div><div id="outline-container-sec-3-1" class="outline-3">
<h3 id="sec-3-1">Model Representation</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Notations:
</p>
<ol class="org-ol">
<li>\(m\) is Number of training examples.
</li>
<li>\(x\)'s is "input" variable  / feature.
</li>
<li>\(y\)'s is "output" variable / "target" variables.
</li>
<li>\((x,y)\) is one training example.
</li>
<li>\((x^{(i)},y^{(i)})\) is the \(i\)th example, \(i\) starts from \(0\).
</li>
<li>\(h\) is called hypothesis, it maps the input to output. In this lecture, we represent \(h\) using linear function, thus it's called <b>linear regression</b>. For linear regression with one variable, it's called <b>univariate linear regression</b>. For example, the <b>univariate linear regression</b> is usually written as: \(h_\theta (x) = \theta_0 + \theta_1 x\). The \(\theta\) here represents the coefficient variables. Sometimes it's written as \(h(x)\) as a shorthand.
</li>
</ol>
</div>
</div>
<div id="outline-container-sec-3-2" class="outline-3">
<h3 id="sec-3-2">Cost Function</h3>
<div class="outline-text-3" id="text-3-2">
<p>
Since the hypothesis is written as \(h_\theta (x) = \theta_0 + \theta_1 x\), where \(\theta_i\)'s are the parameters, then how to choose \(\theta_i\)'s?
The idea is to choose \(\theta_0, \theta_1\) so that \(h_\theta (x)\) is close to \(y\) for our training examples \((x,y)\). More formally, we want to
\[
\underset{\theta_0,\theta_1}{\text{minimize}} \frac{1}{2m}\sum_{i=1}^{m}\left( h_\theta(x^{(i)}) - y^{(i)} \right)^2
\],
where \(m\) is the number of trainnig examples. To recap, we're minimizing half of the averaging error. Note that the variable here are \(\theta\)s, and \(x\) and \(y\)'s are constants.
</p>

<p>
By convention, we define the <b>cost function</b> \(J(\theta_0,\theta_1)\) to represent the objective function. That is
</p>
\begin{gather*}
J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^{m}\left( h_\theta(x^{(i)}) - y^{(i)} \right)^2 \\
\underset{\theta_0,\theta_1}{\text{minimize}} J(\theta_0,\theta_1)
\end{gather*}

<p>
This cost function is also called <b>squared error function</b>. There are other cost functions, but it turns out that squared error function is a resonable choice and will work for most of regression problem.
</p>
</div>
</div>
<div id="outline-container-sec-3-3" class="outline-3">
<h3 id="sec-3-3">Cost Function Intuition</h3>
<div class="outline-text-3" id="text-3-3">
<p>
Before getting a intuition about the cost function, let's have a recap, we now have
</p>
<ol class="org-ol">
<li>Hypothesis: \[h_\theta (x) = \theta_0 + \theta_1 x\]
</li>
<li>Parameters: \[\theta_0,\theta_1\]
</li>
<li>Cost Function: \[J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^{m}\left( h_\theta(x^{(i)}) - y^{(i)} \right)^2 \]
</li>
<li>Goal: \[\underset{\theta_0,\theta_1}{\text{minimize}} J(\theta_0,\theta_1)\]
</li>
</ol>

<p>
In order to visualize our cost function, we use a simplified hypothesis function: \(h_\theta (x) = \theta_1 x\), which sets \(\theta_0\) to \(0\). So now we have
</p>
<ol class="org-ol">
<li>Hypothesis: \[h_\theta (x) =  \theta_1 x\]
</li>
<li>Parameters: \[\theta_1\]
</li>
<li>Cost Function: \[J(\theta_1) = \frac{1}{2m}\sum_{i=1}^{m}\left( h_\theta(x^{(i)}) - y^{(i)} \right)^2 \]
</li>
<li>Goal: \[\underset{\theta_1}{\text{minimize}} J(\theta_1)\]
</li>
</ol>

<p>
So now let's compare function \(h_\theta (x)\) and function \(J(\theta_1)\):
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-05.png" alt="screenshot-05.png" />
</p>

<p>
Then let's come back to the original function, where we don't have the constrain that \(\theta_0 = 0\). The comparison is like
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-06.png" alt="screenshot-06.png" />
</p>
</div>
</div>
<div id="outline-container-sec-3-4" class="outline-3">
<h3 id="sec-3-4">Gradient Descent</h3>
<div class="outline-text-3" id="text-3-4">
<p>
Now we have some function \(J(\theta_0,\theta_1)\) and we want to \(\underset{\theta_0,\theta_1}{\text{minimize}}J(\theta_0,\theta_1)\), we use <b>gradient descent</b> here, which
</p>
<ol class="org-ol">
<li>Start with some \(\theta_0,\theta_1\),
</li>
<li>Keep changing \(\theta_0,\theta_1\) to reduce \(J(\theta_0,\theta_1)\), until we hopefully end up at a minimum.
</li>
</ol>

<p>
To help understand gradient descent, suppose you are standing at one point on the hill, and you want to take a small step to step downhill as quickly as possible, then you would choose the deepest direction to downhill.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-07.png" alt="screenshot-07.png" />
You keep doing this until to get to a local minimum.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-08.png" alt="screenshot-08.png" />
</p>

<p>
But if you start with a different initial position, gradient descent will take you to a (very) different position.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-08.png" alt="screenshot-08.png" />
</p>
</div>
<div id="outline-container-sec-3-4-1" class="outline-4">
<h4 id="sec-3-4-1">Gradient Descent algorithm</h4>
<div class="outline-text-4" id="text-3-4-1">
<p>
We use \(a := b\) to represent <b>assignment</b> and \( a = b\) to represent <b>truth assertion</b>.
</p>




<div class="figure">
<p><img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/test.png" alt="/assets/blog/2015/01/16/Stanford Machine Learning Notes/test.png" />
</p>
</div>


<p>
The \(\alpha\) here is called learning rate.
</p>

<p>
Pay attention that when implementing gradient descent, we need to update all \(\theta\text{s}\) simultaneous.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-10.png" alt="screenshot-10.png" />
</p>

<p>
Recall that \(\alpha\) is a called the learning rate, which is actually a scale factor to our step represented by the derivative term. Take a 1D case as an example, the derivative is the direction (slop of the tanget line) where the function value becomes larger, so we should take its negative as our step.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-11.png" alt="screenshot-11.png" />
</p>


<p>
If \(\alpha\) is too small, gradient descent can be slow. If \(\alpha\) is too large, gradient can overshoot the minimum. It may fail to converge, or even diverge.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-12.png" alt="screenshot-12.png" />
</p>


<p>
Gradient descent can converge to a local minimum, even with the learning rate \(\alpha\) fixed. This is because as we approach a local minimum, gradient descent will automatically take smaller steps. So no need to descrease \(\alpha\) over time.
</p>


<p>
<b>Batch</b> Gradient descent:
Each step of gradient descent uses all the training examples:
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4">Linear Algebra (Week1, Optional)</h2>
<div class="outline-text-2" id="text-4">
<p>
This lecture use 1-indexed subscripts.
</p>
</div>
</div>
<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5">Linear Regression with Multiple Variables (Week 2)</h2>
<div class="outline-text-2" id="text-5">
</div><div id="outline-container-sec-5-1" class="outline-3">
<h3 id="sec-5-1">Multiple Features</h3>
<div class="outline-text-3" id="text-5-1">
<p>
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-13.png" alt="screenshot-13.png" />
Notation:
</p>
<ol class="org-ol">
<li>number of features: \(n\)
</li>
<li>input (features) of \(i^{\text{th}}\) training example: \(x^{(i)}\)
</li>
<li>value of feature \(j\) in \(i^{th}\) training example. \(x^{(i)}_{j}\)
</li>
</ol>

<p>
Now our hypothesis is \(h_\theta (x) = \theta_0 + \theta_1 x_1 +  \theta_2 x_2 + \dots + \theta_n x_n \).
For convenience of notation, define \(x_0 = 1\), or \(x^{(i)}_0 = 1\). So now we define our hypothesis as
</p>
\begin{equation*}
\begin{split}
h_\theta (x) &= \theta_0 x_0 + \theta_1 x_1 +  \theta_2 x_2 + \dots + \theta_n x_n \\
&= \theta x
\end{split}
\end{equation*}
<p>
where \(\theta\) is a \(n+1\) dimension vector.
</p>
</div>
</div>
<div id="outline-container-sec-5-2" class="outline-3">
<h3 id="sec-5-2">Gradient Descent for Multiple Variables</h3>
<div class="outline-text-3" id="text-5-2">
<p>
So now our new algorithm becomes
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-14.png" alt="screenshot-14.png" />
Note that our new algorithm for \(\theta_0\) is just like the old one since \(x_0^{(i)} \) is 1.
</p>
</div>
</div>
<div id="outline-container-sec-5-3" class="outline-3">
<h3 id="sec-5-3">Gradient Descent in Practice - Feature Scaling.</h3>
<div class="outline-text-3" id="text-5-3">
<p>
Idea: Make sure features are on a similar scale, then gradient descent will converge more quickly.
</p>

<p>
Take a 2D example,  if the dimension of \(x_1\) is much larger than the dimension of \(x_2\), then the search region is a long ellipsis shape which is make gradient much difficult to find the minimum.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-15.png" alt="screenshot-15.png" />
We can rescale all features to [0,1] so the contours now become a circle.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-16.png" alt="screenshot-16.png" />
</p>

<p>
Usually we get every feature into approximately a \(-1 \leq x \leq 1\) range. But it need not to be exactly. Say \(-3 \leq x \leq 3 \) is OK.
</p>


<p>
Mean normalization: Replace \(x_i\) with \(x_i - \mu_i\) to make features have approximately zero mean. Note that this does not apply to \(x_0 = 1\).
</p>
</div>
</div>
<div id="outline-container-sec-5-4" class="outline-3">
<h3 id="sec-5-4">Gradient Descent in Practice - Learning Rate.</h3>
<div class="outline-text-3" id="text-5-4">
<p>
To make sure gradient descent is working correctly, draw the figure the value of \(J(\theta)\) versus the number of iterations.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-17.png" alt="screenshot-17.png" />
</p>

<p>
For sufficiently small \(\alpha\), \(J(\theta\) should decrease on every iteration. But if \(\alpha\)  is too small, gradient descent can converge too slow. To choose \(\alpha\) try \( \dots, 0.001, 0.003,0.01, 0.03,0.1, 0.3,1, \dots\), roughly a 3x larger.
</p>
</div>
</div>
<div id="outline-container-sec-5-5" class="outline-3">
<h3 id="sec-5-5">Features and Polynomial Regression</h3>
<div class="outline-text-3" id="text-5-5">
<p>
You need to choose a good feature instead of just using what you're provided. For example, for housing price prediction, you are provided with the frontage and depth feature, you can define a feature called area = frontage x depth.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-18.png" alt="screenshot-18.png" />
</p>

<p>
Or sometimes use a polynomial function would be better. If the feature is not enough, you could use size, size^2, size^3 as features.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-18.png" alt="screenshot-18.png" />
</p>

<p>
Or you can use square root as feature.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-19.png" alt="screenshot-19.png" />
How to find the minimum of \(J(\theta)\) analytically?
</p>

<p>
By Calculus, we can take the partial derivatives of each variable, and set it to 0.
</p>

<p>
Normal Equation:
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-22.png" alt="screenshot-22.png" />
Then we can compute \(\theta = (X^T X)^{-1} X^T y\)
The Octave code is:
</p>
<div class="org-src-container">

<pre class="src src-octave">pinv(X' * X) * X' * y
</pre>
</div>
<p>
Compare Gradient Descent with Normal Equation.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-20.png" alt="screenshot-20.png" />
</p>
</div>
</div>
<div id="outline-container-sec-5-6" class="outline-3">
<h3 id="sec-5-6">Normal Equation and Noninvertbility (Optional)</h3>
<div class="outline-text-3" id="text-5-6">
<p>
Use <code>pinv</code> in Octave should not be a problem.
What if \(X^T X\) is non-invertible?
</p>
<ol class="org-ol">
<li>Reduent features (linearyly dependant).
E.g. \(x_1\) = size in feet, \(x_2\) size in m^2.
</li>
<li>Too many features (e.g. $m &le; n).
Delete some features, or use regulariation.
</li>
</ol>
</div>
</div>
</div>



<div id="outline-container-sec-6" class="outline-2">
<h2 id="sec-6">Octave Tutorial</h2>
<div class="outline-text-2" id="text-6">
</div><div id="outline-container-sec-6-1" class="outline-3">
<h3 id="sec-6-1">Basic Operation</h3>
<div class="outline-text-3" id="text-6-1">
<div class="org-src-container">

<pre class="src src-octave">5+6
3-2
5*8
1/2
2^6


1 == 2 %false
1 ~= 2 
1 &amp;&amp; 0
1 || 0

xor(1,0) %XOR

PS1('&gt;&gt; '); % to change the prompt sign

a = 3; % semicolon supressing output
a

disp(a);
disp(sprintf('2 decimals: %0.2f', a))

a
format long
a
format short
a

A = [1 2; 3 4; 5 6]

A = [1 2
     3 4
     5 6]

v = [1 2 3]

v = 1:0.1:2

ones(2,3)

C = 2*ones(2,3)

C = [2 2 2; 2 2 2]

w = ones(1,3)
w = zeros(1,3)
w = rand(1,3)

rand(3,3)
rand(3,3)

w = randn(1,3) % guassian distribution

w = -6 + sqrt(10)*(randn(1,10000)); % you don't want to omit the semicolon here

hist(w)
hist(w,50)

eye(4) % identity matrix
I = eye(6)

help eye
</pre>
</div>


<div class="figure">
<p><img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/chart.png" alt="/assets/blog/2015/01/16/Stanford Machine Learning Notes/chart.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-6-2" class="outline-3">
<h3 id="sec-6-2">Move data around</h3>
<div class="outline-text-3" id="text-6-2">
<div class="org-src-container">

<pre class="src src-octave">A = [1 2; 3 5; 5 6]
size(A)
sz = size(A)
size(A,1)
v = [1 2 3 4]
length(v)
length(A)
length([1;2;3;4;5])
pwd
cd ..
ls
load('test.dat')
who
whos
save hello.mat v
save hello.txt v -ascii
clear
A = [1 2 ; 3 4; 5 6];
A(3,2)
A(:,2)
A([1 3], :)
A(:,2) = [10; 11; 12]
A = [A, [ 100, 101, 102]];
A(:) % put all elements of A into a single vector
A = [1 2; 3 4; 5 7];
B = [11 12; 13 14; 15 16];
C = [A B]
C = [A,B]
D = [A; B]
</pre>
</div>
</div>
</div>
<div id="outline-container-sec-6-3" class="outline-3">
<h3 id="sec-6-3">Computing on Data</h3>
<div class="outline-text-3" id="text-6-3">
<div class="org-src-container">

<pre class="src src-octave">A = [1 2; 3 4; 5 6];
B = [11 12; 13 14; 15 16];
C = [11 12; 13 14]

A.*B
A.^2
v = [1; 2; 3]
1 ./ v
1 ./ A
log(v)
exp(v)
abs(v)
abs([-1; 2; -3])
-v
v + ones(length(v),1)
v + 1

A'
(A')'

a = [1 15 2 0.5]
val = max(a)
[val, ind] = max(a)
max(A)
a &lt; 3
A = magic(3)
[r,c] = find(A &gt;= 7)

sum(a)
prod(a)
floor(a)
ceil(a)

max(A,[],1) % colomn wise max
max(A,[],2) % row wise max
max(A)
max(max(A))
max(A(:)) % find max of all the elements

A = magic(9)
sum(A,1) % column wise sum

sum(sum(A .* (eye(9)))) % sum the diagonal values
sum(sum(A .*flipud(eye(9)))) % sum the subdiagonal values

pinv(A)                         # sudo inverse
temp = pinv(A)
temp * A
</pre>
</div>
</div>
</div>
<div id="outline-container-sec-6-4" class="outline-3">
<h3 id="sec-6-4">Plotting Data</h3>
<div class="outline-text-3" id="text-6-4">
<div class="org-src-container">

<pre class="src src-octave">t = [0:0.01:0.98];
y1 = sin(2*pi*4*t);
plot(t,y1);
y2 = cos(2*pi*4*t);
plot(t,y2);

plot(t,y1);
hold on;
plot(t,y2,'r');
xlabel('time');
ylabel('value');
legend('sin','cos');
title('my plot');
print -dpng 'myPlot.png'
close


figure(1); plot(t,y1);
figure(2); plot(t,y2);

subplot(1,2,1)% divides plot into a 1x2 grid, access first element
plot(t,y1);
subplot(1,2,2);
plot(t,y2);
axis([0.5 1 -1 1]) % change horizontal range to [0.5,1] and vertical to [-1,1]

A = magic(5)
imagesc(A)
imagesc(A), colorbar, colormap gray; % use comma for command chainning, for ouput, which is different from semicolon
</pre>
</div>
</div>
</div>
<div id="outline-container-sec-6-5" class="outline-3">
<h3 id="sec-6-5">Control Statements</h3>
<div class="outline-text-3" id="text-6-5">
<div class="org-src-container">

<pre class="src src-octave">v = zeros(10,1)
for i = 1:10,
    v(i) = 2^i;
end;

i = 1;
while i &lt;= 5,
      v(i) = 100;
end


i = 1;
while true,
      v(i) = 999;
      i = i + 1;
      if( i == 6),
        break;
      end;
end;
</pre>
</div>
</div>
</div>
<div id="outline-container-sec-6-6" class="outline-3">
<h3 id="sec-6-6">Vectorizatrion</h3>
<div class="outline-text-3" id="text-6-6">
<p>
\(h_\theta(x) = \sum_{j=0}^{n} \theta_j x_j = \theta^T x\)
</p>
<div class="org-src-container">

<pre class="src src-octave">%unvectorized implemenetation
  predictaion = 0.0;
  for j = 1;n+1,
      prediction = prediction + theta(j) * x(j)
  end;
  %vectorized implementation
  prediction = thteta' * x;
</pre>
</div>
</div>
</div>
</div>




<div id="outline-container-sec-7" class="outline-2">
<h2 id="sec-7">Logistic Regression (Week 3)</h2>
<div class="outline-text-2" id="text-7">
</div><div id="outline-container-sec-7-1" class="outline-3">
<h3 id="sec-7-1">Classification (8min)</h3>
<div class="outline-text-3" id="text-7-1">
<p>
Applying linear regression to a classification problem is not a good idea.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-23.png" alt="screenshot-23.png" />
</p>

<p>
You can use 0.5 as a threshhold to do the prediction based on the h_&amp;theta;(x)
value. However, this is not a good idea since when adding a new sample point,
the hypothesis will change. Besides, the return value for h_&amp;theta;(x) could be
not in the range [0, 1], thus making the prediction rather confusing. Logistic
Regression, though the word regression, is used to do the classification job and
the hypothesis can be guaranteed in the range [0,1].
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-24.png" alt="screenshot-24.png" />
</p>
</div>
</div>
<div id="outline-container-sec-7-2" class="outline-3">
<h3 id="sec-7-2">Hypothesis Representation</h3>
<div class="outline-text-3" id="text-7-2">
</div><div id="outline-container-sec-7-2-1" class="outline-4">
<h4 id="sec-7-2-1">Logistic Regression Model</h4>
<div class="outline-text-4" id="text-7-2-1">
<p>
We set \(h_\theta(x) = g(\theta^Tx)\), where \(g(z) = \frac{1}{1+e^{-z}}\). The \(g\)
function is called <b>Sigmoid function</b> as well as <b>Logistic function</b>.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-25.png" alt="screenshot-25.png" />
</p>
</div>
</div>
<div id="outline-container-sec-7-2-2" class="outline-4">
<h4 id="sec-7-2-2">Interpretation of Hypothesis Output</h4>
<div class="outline-text-4" id="text-7-2-2">
<p>
You can think of \(h_\theta(x)\) as the estimated probability that \(y = 1\) on
input \(x\).
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-26.png" alt="screenshot-26.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-sec-7-3" class="outline-3">
<h3 id="sec-7-3">Decision Boundary</h3>
<div class="outline-text-3" id="text-7-3">
<p>
It's a line that separates the regions where the hypothesis predicts 1 or 0.
</p>

<p>
Since \(g(z) \geq 0.5\) when \(z \geq 0.5\), which means that \(h_\theta(x) \geq 0.5\) whenever
\(\theta^{T} x \geq 0\)
</p>
</div>
</div>
<div id="outline-container-sec-7-4" class="outline-3">
<h3 id="sec-7-4">Cost Function</h3>
<div class="outline-text-3" id="text-7-4">
<p>
How to fit the parameters &theta; for Logistic Regression?
We define the cost function as below since \(h_\theta(x)\) is in the range [0,1].
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-27.png" alt="screenshot-27.png" />
</p>
</div>
</div>
<div id="outline-container-sec-7-5" class="outline-3">
<h3 id="sec-7-5">Simplified Cost Function and Gradient Descent</h3>
<div class="outline-text-3" id="text-7-5">
<p>
Remember that the Logistic regression cost function is in two parts, since y can be 0 or 1 only,  we can write the cost function in a new way:
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-28.png" alt="screenshot-28.png" />
</p>

<p>
Remember that gradient descent is like:
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-29.png" alt="screenshot-29.png" />
</p>

<p>
where the partial derivatives is like:
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-30.png" alt="screenshot-30.png" />
</p>
</div>
</div>
<div id="outline-container-sec-7-6" class="outline-3">
<h3 id="sec-7-6">Advance Optimization</h3>
<div class="outline-text-3" id="text-7-6">

<div class="figure">
<p><img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-31.png" alt="screenshot-31.png" />
</p>
</div>

<p>
Note that the Octave start from 1 instead of 0.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-32.png" alt="screenshot-32.png" />
</p>
</div>
</div>
<div id="outline-container-sec-7-7" class="outline-3">
<h3 id="sec-7-7">Multiclass Classification: One-vs-all</h3>
<div class="outline-text-3" id="text-7-7">
<p>
One-vs-all is also called one-vs-rest.
</p>


<div class="figure">
<p><img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-33.png" alt="screenshot-33.png" />
</p>
</div>


<p>
The method is:
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-34.png" alt="screenshot-34.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-sec-8" class="outline-2">
<h2 id="sec-8">Regulation (Week 3)</h2>
<div class="outline-text-2" id="text-8">
</div><div id="outline-container-sec-8-1" class="outline-3">
<h3 id="sec-8-1">The problem of Overfitting</h3>
<div class="outline-text-3" id="text-8-1">

<div class="figure">
<p><img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-35.png" alt="screenshot-35.png" />
</p>
</div>


<div class="figure">
<p><img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-36.png" alt="screenshot-36.png" />
</p>
</div>

<p>
For Logistic regression:
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-37.png" alt="screenshot-37.png" />
</p>

<p>
How to solve the problem?
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-38.png" alt="screenshot-38.png" />
</p>
</div>
</div>
<div id="outline-container-sec-8-2" class="outline-3">
<h3 id="sec-8-2">Cost Function</h3>
<div class="outline-text-3" id="text-8-2">
<p>
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-39.png" alt="screenshot-39.png" />
The idea behind regulation is that:
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-40.png" alt="screenshot-40.png" />
Note that we didn't penalize &theta;_0
</p>


<div class="figure">
<p><img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-41.png" alt="screenshot-41.png" />
</p>
</div>

<p>
What if &lambda; is too large?
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-42.png" alt="screenshot-42.png" />
</p>
</div>
</div>
<div id="outline-container-sec-8-3" class="outline-3">
<h3 id="sec-8-3">Regularized linear regression</h3>
<div class="outline-text-3" id="text-8-3">

<div class="figure">
<p><img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-43.png" alt="screenshot-43.png" />
</p>
</div>

<p>
When using normal equation:
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-44.png" alt="screenshot-44.png" />
</p>
</div>
</div>
<div id="outline-container-sec-8-4" class="outline-3">
<h3 id="sec-8-4">Regularized Logistic Regression</h3>
<div class="outline-text-3" id="text-8-4">
<p>
Now update &theta;_0 separately
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-45.png" alt="screenshot-45.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-sec-9" class="outline-2">
<h2 id="sec-9">Neural Networks: Representation</h2>
<div class="outline-text-2" id="text-9">
</div><div id="outline-container-sec-9-1" class="outline-3">
<h3 id="sec-9-1">Non-linear Hypotheses</h3>
<div class="outline-text-3" id="text-9-1">
<p>
Use quadratic features will results in a lot of features.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-46.png" alt="screenshot-46.png" />
</p>
</div>
</div>
<div id="outline-container-sec-9-2" class="outline-3">
<h3 id="sec-9-2">Neurons and the Brain</h3>
<div class="outline-text-3" id="text-9-2">
<p>
There is one learning algorithm of the brain
</p>
</div>
</div>
<div id="outline-container-sec-9-3" class="outline-3">
<h3 id="sec-9-3">Model Representation I</h3>
<div class="outline-text-3" id="text-9-3">
<p>
Neuron model: Logistic unit
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-47.png" alt="screenshot-47.png" />
</p>

<p>
Neural Network is a group of this neural.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-48.png" alt="screenshot-48.png" />
</p>



<div class="figure">
<p><img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-49.png" alt="screenshot-49.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-sec-9-4" class="outline-3">
<h3 id="sec-9-4">Model Representation II</h3>
<div class="outline-text-3" id="text-9-4">
<p>
Forward propagation: Vectorized implementation
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-50.png" alt="screenshot-50.png" />
</p>

<p>
Bias unit:
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-51.png" alt="screenshot-51.png" />
</p>

<p>
The last layer is like Logistic Regression, but Neural Network learning its own
features.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-52.png" alt="screenshot-52.png" />
</p>
</div>
</div>
<div id="outline-container-sec-9-5" class="outline-3">
<h3 id="sec-9-5">Examples and Intuitions I</h3>
<div class="outline-text-3" id="text-9-5">
<p>
OR Function
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-53.png" alt="screenshot-53.png" />
</p>
</div>
</div>
<div id="outline-container-sec-9-6" class="outline-3">
<h3 id="sec-9-6">Examples and Intuitions II</h3>
<div class="outline-text-3" id="text-9-6">
<p>
NOT Function
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-54.png" alt="screenshot-54.png" />
</p>

<p>
The input layer has only x_1 and x_2. The second layer compute the feature (x_1 AND x_2), and the feature (NOT x_1) AND (NOT x_2). The third layer use the feature computed in layer 2 and do an OR operation to simulates a XNOR operation.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-55.png" alt="screenshot-55.png" />
</p>


<p>
Handwritten digit classification
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-56.png" alt="screenshot-56.png" />
</p>
</div>
</div>
<div id="outline-container-sec-9-7" class="outline-3">
<h3 id="sec-9-7">Mutilcass Classification</h3>
<div class="outline-text-3" id="text-9-7">
<p>
Use an extension of the One-vs-all method.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-57.png" alt="screenshot-57.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-sec-10" class="outline-2">
<h2 id="sec-10">Neural Networks: Learning (Week 5)</h2>
<div class="outline-text-2" id="text-10">
</div><div id="outline-container-sec-10-1" class="outline-3">
<h3 id="sec-10-1">Cost Function</h3>
<div class="outline-text-3" id="text-10-1">

<div class="figure">
<p><img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-58.png" alt="screenshot-58.png" />
</p>
</div>

<p>
This is the cost function
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-59.png" alt="screenshot-59.png" />
</p>
</div>
</div>
<div id="outline-container-sec-10-2" class="outline-3">
<h3 id="sec-10-2">Backpropagation Algorithm</h3>
<div class="outline-text-3" id="text-10-2">
<p>
Forward Propagation, computing all the activations:
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-60.png" alt="screenshot-60.png" />
</p>

<p>
Backpropagation algorithm, computing the gradient:
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-61.png" alt="screenshot-61.png" />
</p>


<div class="figure">
<p><img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-62.png" alt="screenshot-62.png" />
</p>
</div>

<p>
The name "back" comes from the order we compute the "error"
Backpropagation algorithm
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-63.png" alt="screenshot-63.png" />
</p>
</div>
</div>
<div id="outline-container-sec-10-3" class="outline-3">
<h3 id="sec-10-3">Backpropagation Intuition</h3>
<div class="outline-text-3" id="text-10-3">
<p>
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-64.png" alt="screenshot-64.png" />
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-65.png" alt="screenshot-65.png" />
</p>
</div>
</div>

<div id="outline-container-sec-10-4" class="outline-3">
<h3 id="sec-10-4">Implementation Notes Unrolling Parameters</h3>
<div class="outline-text-3" id="text-10-4">
<p>
In Neural Network our parameters are matrices, and we need to unroll it into vectors.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-66.png" alt="screenshot-66.png" />
</p>
</div>
</div>
<div id="outline-container-sec-10-5" class="outline-3">
<h3 id="sec-10-5">Gradient Checking</h3>
<div class="outline-text-3" id="text-10-5">
<p>
Use nummerical estimation of gradients to check.
</p>
</div>
</div>
<div id="outline-container-sec-10-6" class="outline-3">
<h3 id="sec-10-6">Random Initialization</h3>
<div class="outline-text-3" id="text-10-6">
<p>
We need to provide an initial value for gradient fescent and advanced optimization method.
</p>

<p>
Zero Initialization will result in identical values.
</p>

<p>
To solve this problem, we use random initialization to break the symmerty.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-69.png" alt="screenshot-69.png" />
</p>
</div>
</div>
<div id="outline-container-sec-10-7" class="outline-3">
<h3 id="sec-10-7">Put it together</h3>
<div class="outline-text-3" id="text-10-7">

<div class="figure">
<p><img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-70.png" alt="screenshot-70.png" />
</p>
</div>

<p>
How to train a neural network
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-71.png" alt="screenshot-71.png" />
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-72.png" alt="screenshot-72.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-sec-11" class="outline-2">
<h2 id="sec-11">Advice for Applying machine Learning</h2>
<div class="outline-text-2" id="text-11">
</div><div id="outline-container-sec-11-1" class="outline-3">
<h3 id="sec-11-1">Deciding What to Try Next</h3>
<div class="outline-text-3" id="text-11-1">
<ol class="org-ol">
<li>Get more training examples
</li>
<li>Try smaller sets of features
</li>
<li>Try getting additional features.
</li>
<li>Try adding polynomial features
</li>
<li>Try decreasing &lambda;.
</li>
<li>Try increasing &lambda;.
</li>
</ol>
</div>
</div>
<div id="outline-container-sec-11-2" class="outline-3">
<h3 id="sec-11-2">Evaluating a Hypothesis</h3>
<div class="outline-text-3" id="text-11-2">
<p>
Hypothesis can be overfitting.
</p>

<p>
We can evaluate our hypothesis by separate our date into training set and test set.
</p>
</div>
</div>
<div id="outline-container-sec-11-3" class="outline-3">
<h3 id="sec-11-3">Model Selection and Train/Validation/Test Sets</h3>
<div class="outline-text-3" id="text-11-3">

<div class="figure">
<p><img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-73.png" alt="screenshot-73.png" />
</p>
</div>

<p>
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-74.png" alt="screenshot-74.png" />
To address this problem, we divide our data set into three parts: training set, cross validation set, and test set.
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-76.png" alt="screenshot-76.png" />
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-75.png" alt="screenshot-75.png" />
</p>

<p>
Use cross validation error to selection the model degree and use test set error to select &theta;.
</p>
</div>
</div>
<div id="outline-container-sec-11-4" class="outline-3">
<h3 id="sec-11-4">Diagonosing Bias vs Variance</h3>
<div class="outline-text-3" id="text-11-4">
<p>
Bias (undearfit)
Variance (overfit)
<img src="/assets/blog/2015/01/16/Stanford Machine Learning Notes/screenshot-77.png" alt="screenshot-77.png" />
</p>
</div>
</div>
</div>

</div>
</div>
    <div>
      <div class="post-meta">
        <span title="post date" class="post-info">2015-01-16</span>
        <span title="last modification date" class="post-info">2015-03-19</span>
        <span title="tags" class="post-info"><a href="/tags/machine-learning/">Machine Learning</a>, <a href="/tags/notes/">notes</a></span>
        <span title="author" class="post-info">Xiaolong Zhang</span>
      </div>
      <section>
        <h1>Comments</h1>
        <div id="disqus_thread"></div>
        <script type="text/javascript">
          //var disqus_developer = 1;
          var disqus_identifier = "/blog/2015/01/16/Stanford Machine Learning Notes";
          var disqus_url = "http://heartnheart.github.io/blog/2015/01/16/Stanford Machine Learning Notes";
          var disqus_shortname = 'heartnheart';
          /* * * DON'T EDIT BELOW THIS LINE * * */
          (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
          })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      </section>
      <script src="http://code.jquery.com/jquery-latest.min.js"></script>
      <script src="https://google-code-prettify.googlecode.com/svn/loader/prettify.js"></script>
      <script src="/media/js/main.js"></script>
      <script type="text/javascript">
        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-61112071-1']);
        _gaq.push(['_trackPageview']);
        (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();
      </script>
      <div class="footer">
        <p>Generated by <a href="http://www.gnu.org/software/emacs/">Emacs</a> 25.x (<a href="http://orgmode.org">Org mode</a> 8.x)</p>
        <p>
          Copyright &copy; 2012 - 2014 <a href="mailto:xlzhang &lt;at&gt; cs &lt;dot&gt; hku &lt;dot&gt; hk">Xiaolong Zhang</a>
          &nbsp;&nbsp;-&nbsp;&nbsp;
          Powered by <a href="https://github.com/kelvinh/org-page" target="_blank">org-page</a>
        </p>
      </div>
    </div>
  </body>
</html>
